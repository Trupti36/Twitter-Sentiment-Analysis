# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O3IOaMaweyriRCZcQcde83RXX3lhnv7d

## Twitter Sentiment Analysis
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/Data

data=pd.read_csv('twitter.csv')
df

df.drop_duplicates()

data.columns=['ID','label','tweet']

data.describe()

data.info()

import seaborn as sns
import matplotlib.pyplot as plt

tweets_df=data

data['tweet']

"""Get Length"""

tweets_df['length']=tweets_df['tweet'].apply(len)

tweets_df['length'].plot(bins=100, kind='hist')
plt.show()

tweets_df.describe()

"""# Define the sentiments"""

positive = tweets_df[tweets_df['label']==0]
positive

negative = tweets_df[tweets_df['label']==1]
negative

"""# Cleaning"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

nltk.download('stopwords')

nltk.download('punkt')

# Preprocessing function
def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text.lower())
    # Remove stopwords and punctuation
    stop_words = set(stopwords.words('english') + list(string.punctuation))
    filtered_tokens = [token for token in tokens if token not in stop_words and token!= 'user']
    return ' '.join(filtered_tokens)



# Apply preprocessing to the 'Tweet content' column
data['tweet']=data['tweet'].astype('str')
data['clean_text'] = data['tweet'].apply(preprocess_text)

"""# WorldCloud"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

def generate_wordcloud(sentiment):
    # Filter tweets of the specified sentiment and extract the 'Tweet content' column
    tweets = data[data["label"] == sentiment]["tweet"]

    # Convert the tweets to strings
    tweets = tweets.astype(str)

    # Join all the tweets into a single string
    text = ' '.join(tweets)

    # Create a WordCloud object
    wordcloud = WordCloud(
        width=700, height=400,
        colormap='Blues').generate(text)


    # Display the word cloud
    plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'Word Cloud of {sentiment} Sentiment Tweets')
    plt.show()


# Iterate over the unique sentiments in the DataFrame
sentiments = data['label'].unique()
for sentiment in sentiments:
    generate_wordcloud(sentiment)

"""# Train Model"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(data['clean_text'], data['label'], test_size=0.2, random_state=42)

# Vectorize the text data
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_val_vec = vectorizer.transform(X_val)

# Train a Support Vector Machine (SVM) classifier
svm = SVC(kernel='linear')
svm.fit(X_train_vec, y_train)

# Make predictions on the validation set
y_pred = svm.predict(X_val_vec)

# Evaluate the model
accuracy = accuracy_score(y_val, y_pred)
report = classification_report(y_val, y_pred)

print('Accuracy:', accuracy)
print('Classification Report:')
print(report)

data['clean_text'] = data['tweet'].apply(preprocess_text)

# Vectorize the text data
X_val_data = vectorizer.transform(data['clean_text'])

# Make predictions on the validation data
y_val_pred = svm.predict(X_val_data)

# save the predicted sentiments
predictions_df = pd.DataFrame({'Predicted Sentiment': y_val_pred})
predictions_df.to_csv('predicted_sentiments.csv', index=False)

from sklearn.preprocessing import LabelEncoder

# Define the LabelEncoder object
label_encoder = LabelEncoder()

# Fit the LabelEncoder on the sentiment labels in the training data
label_encoder.fit(data['label'])
from sklearn.metrics import confusion_matrix, classification_report

actual_sentiments=data['label']
predicted_sentiments=y_val_pred
# Create a confusion matrix
confusion = confusion_matrix(actual_sentiments, predicted_sentiments)

# Create a classification report
classification = classification_report(actual_sentiments, predicted_sentiments)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, fmt="d", cmap="Greens", cbar=False,
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted Sentiment')
plt.ylabel('Actual Sentiment')
plt.title('Confusion Matrix')
plt.show()

# Print the classification report
print("Classification Report:\n", classification)